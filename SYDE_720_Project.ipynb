{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldeloe/SYDE-720/blob/main/SYDE_720_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pRyXtAiED72"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skimage.measure import block_reduce\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**General Use Functions**"
      ],
      "metadata": {
        "id": "J9LiMCzA9zVP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OydUkKyLjMp_"
      },
      "outputs": [],
      "source": [
        "def generate_scatterplot_indices(dim):\n",
        "  x = []\n",
        "  y = []\n",
        "  for row in range(0,dim):\n",
        "    for col in range(0, dim):\n",
        "      x.append(row)\n",
        "      y.append(col)\n",
        "  return x, y\n",
        "\n",
        "def plot_scatter(x,y,data, title, vmin, vmax):\n",
        "  fig, ax = plt.subplots()\n",
        "  plt.scatter(x,y,c=data, vmin = vmin, vmax = vmax)\n",
        "  cbar = plt.colorbar()\n",
        "  cbar.set_label(\"Magnitude of Velocity Field\")\n",
        "  ax.axes.get_xaxis().set_ticks([])\n",
        "  ax.axes.get_yaxis().set_ticks([])\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "# function that plots training and validation loss\n",
        "def plot_history(history, val_loss):\n",
        "  # plot without validation loss\n",
        "\tif val_loss == False:\n",
        "\t\tplt.plot(history.history['loss'], label='Train')\n",
        "\t# plot with validation loss\n",
        "\telse:\n",
        "\t\tplt.plot(history.history['loss'], label='Train')\n",
        "\t\tplt.plot(history.history['val_loss'], label='Validation')\n",
        "\n",
        "\tplt.xlabel(\"Number of Epochs\")\n",
        "\tplt.ylabel(\"Loss\")\n",
        "\tplt.legend()\n",
        "\tplt.show()\n",
        "\n",
        "def generate_plots(data, filenames, len_train, x, y,output_filename,epochs, dim):\n",
        "  ufield = data[:,:,:,0]\n",
        "  vfield = data[:,:,:,1]\n",
        "  wfield = data[:,:,:,2]\n",
        "  num_fields = 2\n",
        "\n",
        "  normalized_divergence = []\n",
        "\n",
        "  for i in range(0,data.shape[0]*num_fields,num_fields):\n",
        "    idx=int(i/num_fields)\n",
        "\n",
        "    divergence = compute_divergence(data[idx,0:dim,0:dim,:])\n",
        "    print(\"The divergence is: \", (np.abs(np.sum(divergence))/(dim**2)))\n",
        "    normalized_divergence.append([(len_train+idx+1)*100, epochs, np.abs(np.sum(divergence))/(dim**2)])\n",
        "\n",
        "    plot_scatter(x,y,ufield[idx,:,:], 'U Velocity Field', -2.75, 2.75)\n",
        "    plot_scatter(x,y,vfield[idx,:,:], 'V Velocity Field', -2.75, 2.75)\n",
        "    plot_scatter(x,y,wfield[idx,:,:], 'W Velocity Field ',-2.75,2.75)\n",
        "\n",
        "  with open(filepath + output_filename, 'a', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerows(normalized_divergence)\n",
        "\n",
        "def load_data_uv(filepath, filenames, percent, dim):\n",
        "  validation_percent = 0.2\n",
        "  train_fields = []\n",
        "  test_fields = []\n",
        "  train_fieldu = []\n",
        "  train_fieldv = []\n",
        "  train_fieldw = []\n",
        "  test_fieldu = []\n",
        "  test_fieldv = []\n",
        "  test_fieldw = []\n",
        "\n",
        "  train_len = int((len(filenames)/2)*percent)\n",
        "\n",
        "  filenames_train = filenames[0:train_len*2]\n",
        "  filenames_test = filenames[train_len*2:]\n",
        "  val_split = round(len(filenames_train)*validation_percent)\n",
        "  filenames_validation = filenames_train[-val_split:]\n",
        "  filenames_train = filenames_train[:-val_split]\n",
        "\n",
        "  field_z = np.zeros((dim,dim))\n",
        "  block = 4 # corresponds to dim 512x512 downsampled to 128x128\n",
        "  for i in range(0, len(filenames),2):\n",
        "    if i < train_len*2:\n",
        "      train = pd.read_table(filepath + filenames[i], delimiter=',',header=None)\n",
        "      field = train.to_numpy()\n",
        "      field = block_reduce(field, block_size=(block, block), func=np.mean)\n",
        "\n",
        "      train2 = pd.read_table(filepath + filenames[i+1], delimiter=',',header=None)\n",
        "      field2 = train2.to_numpy()\n",
        "      field2 = block_reduce(field2, block_size=(block,block), func=np.mean)\n",
        "\n",
        "      train_fieldu.append(field[0:dim,0:dim])\n",
        "      train_fieldv.append(field2[0:dim,0:dim])\n",
        "      train_fieldw.append(field_z)\n",
        "\n",
        "    else:\n",
        "      test = pd.read_table(filepath + filenames[i], delimiter=',',header=None)\n",
        "      field = test.to_numpy()\n",
        "      field = block_reduce(field, block_size=(block, block), func=np.mean)\n",
        "\n",
        "      test2 = pd.read_table(filepath + filenames[i+1], delimiter=',',header=None)\n",
        "      field2 = test2.to_numpy()\n",
        "      field2 = block_reduce(field2, block_size=(block, block), func=np.mean)\n",
        "\n",
        "      test_fieldu.append(field[0:dim,0:dim])\n",
        "      test_fieldv.append(field2[0:dim,0:dim])\n",
        "      test_fieldw.append(field_z)\n",
        "\n",
        "  train = np.zeros((np.asarray(train_fieldu).shape[0],dim,dim,3))\n",
        "  train[:,:,:,0] = train_fieldu\n",
        "  train[:,:,:,1] = train_fieldv\n",
        "  train[:,:,:,2] = train_fieldw\n",
        "\n",
        "  test = np.zeros((np.asarray(test_fieldu).shape[0],dim,dim,3))\n",
        "  test[:,:,:,0] = test_fieldu\n",
        "  test[:,:,:,1] = test_fieldv\n",
        "  test[:,:,:,2] = test_fieldw\n",
        "\n",
        "  validation = train[-val_split:]\n",
        "  train = train[:-val_split]\n",
        "\n",
        "  return train, test, validation, filenames_train, filenames_test, filenames_validation\n",
        "\n",
        "def compute_divergence(fields):\n",
        "  du_dx, dv_dy, dw_dz = np.gradient(fields)\n",
        "  divergence = du_dx + dv_dy + dw_dz\n",
        "\n",
        "  return divergence\n",
        "\n",
        "def RMSE(train_prediction, test_prediction, train, test):\n",
        "\n",
        "  train_data = train.reshape(train.shape[0], np.prod(train.shape[1:]))\n",
        "  train_pred = train_prediction.reshape(train_prediction.shape[0], np.prod(train_prediction.shape[1:]))\n",
        "  test_data = test.reshape(test.shape[0], np.prod(test.shape[1:]))\n",
        "  test_pred = test_prediction.reshape(test_prediction.shape[0], np.prod(test_prediction.shape[1:]))\n",
        "\n",
        "  trainScore = np.sqrt(mean_squared_error(train_data, train_pred))\n",
        "  print('Training RMSE Score: %.2f' % (trainScore))\n",
        "  testScore = np.sqrt(mean_squared_error(test_data, test_pred))\n",
        "  print('Test RMSE Score: %.2f' % (testScore))\n",
        "\n",
        "def plot_mean_divergence(filenames, input_divergence,epochs,labels):\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(16, 4))\n",
        "  for i in range(0, len(filenames)):\n",
        "    print(filenames[i])\n",
        "    df = pd.read_csv(filepath + filenames[i], header=None)\n",
        "    mean_div = []\n",
        "    for j in epochs:\n",
        "      divergence_at_j = df.loc[df[1] == j]\n",
        "      mean_div.append(sum(divergence_at_j.iloc[:,2])/len(divergence_at_j.iloc[:,2]))\n",
        "\n",
        "    ax.plot(epochs, mean_div, label=labels[i],linestyle='dashed',marker='o')\n",
        "\n",
        "    print(\"Mean divergence at min divergence timestep for 1100: \", mean_div)\n",
        "    min_idx = mean_div.index(min(mean_div))\n",
        "    print(\"Minimum divergence is at epoch \", epochs[min_idx])\n",
        "\n",
        "  ax.hlines(input_divergence[2], xmin=epochs[0], xmax=epochs[-1], label=\"Input field\", color ='k') #, linewidth=2, color='r')\n",
        "\n",
        "  plt.title(\"Total Absolute Divergence for Varied Epochs\" )\n",
        "  plt.xlabel(\"Number of Epochs\")\n",
        "  plt.ylabel(r'$\\sum |\\nabla \\cdot V| [1/s]$')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omN8A8OXu28L"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(7)\n",
        "\n",
        "filepath = \"/content/drive/MyDrive/2D-Turbulence/\"\n",
        "filenames = [\"fileu_100.dat\",\"filev_100.dat\",\n",
        "             \"fileu_200.dat\",\"filev_200.dat\",\n",
        "             \"fileu_300.dat\",\"filev_300.dat\",\n",
        "             \"fileu_400.dat\",\"filev_400.dat\",\n",
        "             \"fileu_500.dat\",\"filev_500.dat\",\n",
        "             \"fileu_600.dat\",\"filev_600.dat\",\n",
        "             \"fileu_700.dat\",\"filev_700.dat\",\n",
        "             \"fileu_800.dat\",\"filev_800.dat\",\n",
        "             \"fileu_900.dat\",\"filev_900.dat\",\n",
        "             \"fileu_1000.dat\",\"filev_1000.dat\",\n",
        "             \"fileu_1100.dat\",\"filev_1100.dat\",\n",
        "             \"fileu_1200.dat\",\"filev_1200.dat\",\n",
        "             \"fileu_1300.dat\",\"filev_1300.dat\",\n",
        "             \"fileu_1400.dat\",\"filev_1400.dat\",\n",
        "             \"fileu_1500.dat\",\"filev_1500.dat\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2O5AgzLdPTI"
      },
      "source": [
        "**Plot the Original Fields for the Test Dataset without Padding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0fhFwjuTTpH"
      },
      "outputs": [],
      "source": [
        "dimensions = 128\n",
        "epochs = 1\n",
        "\n",
        "x, y = generate_scatterplot_indices(dimensions)\n",
        "\n",
        "# 70% training and validation\n",
        "train_percent = 0.7\n",
        "output_filename = \"input_divergence.csv\"\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "generate_plots(test, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename, epochs, dimensions)\n",
        "\n",
        "# 35% training and validation\n",
        "train_percent = 0.35\n",
        "output_filename = \"input_divergence_5_train.csv\"\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "generate_plots(test, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename, epochs,dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmHulITMdEEF"
      },
      "source": [
        "**CAE Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZrpkWbMba2U"
      },
      "outputs": [],
      "source": [
        "def cae_model(train, test, validation, epochs, batch_size, filenames, output_filename,dim):\n",
        "  input = layers.Input(shape=(train.shape[1],train.shape[2], train.shape[3]))\n",
        "\n",
        "  ### encoder ###\n",
        "  encoder_layer1 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(input)\n",
        "  encoder_layer2 = layers.Conv2D(6,  3, 2, activation='relu', padding='same')(encoder_layer1)\n",
        "  encoder_layer3 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(encoder_layer2)\n",
        "\n",
        "  ### decoder ###\n",
        "  decoder_layer1 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(encoder_layer3)\n",
        "  decoder_layer2 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(decoder_layer1)\n",
        "  decoder_layer3 = layers.Conv2DTranspose(3, 4, strides = 2, activation=None, padding=\"same\")(decoder_layer2)\n",
        "\n",
        "  autoencoder = keras.Model(input, decoder_layer3)\n",
        "\n",
        "  autoencoder.compile(optimizer=\"adam\", loss= \"mean_squared_error\")\n",
        "  autoencoder.summary()\n",
        "\n",
        "  history = autoencoder.fit(train, train, epochs=epochs, batch_size=batch_size, validation_data=(validation, validation))\n",
        "  plot_history(history,True)\n",
        "\n",
        "  # model predictions\n",
        "  train_prediction = autoencoder.predict(train)\n",
        "  test_prediction = autoencoder.predict(test)\n",
        "\n",
        "  # calculate RMSE train and test scores\n",
        "  RMSE(train_prediction, test_prediction, train, test)\n",
        "\n",
        "  # plot results\n",
        "  x, y = generate_scatterplot_indices(test.shape[1])\n",
        "  generate_plots(test_prediction, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename,epochs,dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLfKhDyZ1HQd"
      },
      "outputs": [],
      "source": [
        "epochs = [15, 50, 100, 150, 200, 300, 400, 500, 600, 700, 800, 900, 1000,250,350,450,550,650,750,850,950,1050,1100,1150,1200,1250,1300,1350,1400,1450,1550,1600,1650,1700,1750,1800,1850,1900,1950]\n",
        "\n",
        "### cae without zero padding ###\n",
        "dimensions = 128\n",
        "\n",
        "# 70% training and validation dataset\n",
        "train_percent = 0.7\n",
        "output_filename = 'cae_divergence.csv'\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "for i in range(0,len(epochs)):\n",
        "  cae_model(train, test, validation, epochs[i], batch_size, filenames_test,output_filename,dimensions)\n",
        "\n",
        "# 35% training and validation dataset\n",
        "train_percent = 0.35\n",
        "output_filename = 'cae_divergence_5_train.csv'\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "for i in range(0,len(epochs)):\n",
        "  cae_model(train, test, validation, epochs[i], batch_size, filenames_test,output_filename,dimensions)\n",
        "\n",
        "### cae with zero padding ###\n",
        "dimensions = 127\n",
        "\n",
        "# 70% training and validation dataset\n",
        "output_filename = \"cae_divergence_padded.csv\"\n",
        "train_percent = 0.7\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# apply zero padding\n",
        "padding = 1\n",
        "train = np.pad(train, ((0,0),(padding,0),(0,padding),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(padding,0),(0,padding),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(padding,0),(0,padding),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  cae_model(train, test, validation, epochs[i], batch_size, filenames_test,output_filename,dimensions)\n",
        "\n",
        "# 35% training and validation dataset\n",
        "output_filename = \"cae_divergence_padded_5_train.csv\"\n",
        "train_percent = 0.35\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# apply zero padding\n",
        "padding = 1\n",
        "train = np.pad(train, ((0,0),(padding,0),(0,padding),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(padding,0),(0,padding),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(padding,0),(0,padding),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  cae_model(train, test, validation, epochs[i], batch_size, filenames_test,output_filename,dimensions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtOBf2UyWY2c"
      },
      "source": [
        "**Phys CAE Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzhOP4eXee__"
      },
      "outputs": [],
      "source": [
        "class PeriodicBoundaryConditions(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    padded = []\n",
        "\n",
        "    for i in range(0,inputs.shape[3]):\n",
        "        col = inputs[slice(None),1:,:1,i]\n",
        "        col = tf.transpose(col, perm=[1, 2, 0])\n",
        "\n",
        "        crop_input = inputs[slice(None),1:,:-1,i]\n",
        "        crop_input = tf.transpose(crop_input, perm=[1, 2, 0])\n",
        "        pad_col = tf.concat([crop_input,col],1)\n",
        "        row = pad_col[-1:,:,:]\n",
        "\n",
        "        pad_row = tf.concat([row, pad_col],0)\n",
        "        padded.append(pad_row)\n",
        "\n",
        "    padded = tf.transpose(tf.convert_to_tensor(padded), perm=[3, 1, 2, 0])\n",
        "\n",
        "    return padded\n",
        "\n",
        "def kernel_ijk(shape, dtype=None):\n",
        "  delta = 1 # grid spacing\n",
        "  kernel = []\n",
        "\n",
        "  # the kernel is as follows:\n",
        "  # [0, d/dz, -d/dy]\n",
        "  # [-d/dz, 0, d/dx]\n",
        "  # [d/dy, -d/dx, 0]\n",
        "\n",
        "  ### row 1 ###\n",
        "  # 0\n",
        "  kernel.append([[0, 0, 0],\n",
        "                [0, 0, 0],\n",
        "                [0, 0, 0]])\n",
        "  # dA_y/dz\n",
        "  kernel.append([[-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)]])\n",
        "\n",
        "  # -dA_z/dy\n",
        "  kernel.append([[1/(2*delta), 1/(2*delta), 1/(2*delta)],\n",
        "                [0, 0, 0],\n",
        "                [-1/(2*delta), -1/(2*delta), -1/(2*delta)]])\n",
        "\n",
        "  ### row 2 ###\n",
        "  # -dA_x/dz\n",
        "  kernel.append([[1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)]])\n",
        "  # 0\n",
        "  kernel.append([[0, 0, 0],\n",
        "                [0, 0, 0],\n",
        "                [0, 0, 0]])\n",
        "  # dA_z/dx\n",
        "  kernel.append([[-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)]])\n",
        "\n",
        "  ### row 3 ###\n",
        "  # dA_x/dy\n",
        "  kernel.append([[-1/(2*delta), -1/(2*delta), -1/(2*delta)],\n",
        "                [0, 0, 0],\n",
        "                [1/(2*delta), 1/(2*delta), 1/(2*delta)]])\n",
        "  # -dA_y/dx\n",
        "  kernel.append([[1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)]])\n",
        "  # 0\n",
        "  kernel.append([[0, 0, 0],\n",
        "                [0, 0, 0],\n",
        "                [0, 0, 0]])\n",
        "\n",
        "  kernel = np.asarray(kernel).reshape(shape[2],shape[3],shape[0],shape[1])\n",
        "\n",
        "  return kernel\n",
        "\n",
        "def phys_cae(train, test, validation, epochs, batch_size, filenames_train, filenames_test, output_filename,dim):\n",
        "\n",
        "  # image contains zero padding in place of the ghost cells\n",
        "  input = layers.Input(shape=(train.shape[1],train.shape[2], train.shape[3]))\n",
        "\n",
        "  ### encoder ###\n",
        "  encoder_layer1 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(input)\n",
        "  encoder_layer2 = layers.Conv2D(6,  3, 2, activation='relu', padding='same')(encoder_layer1)\n",
        "  encoder_layer3 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(encoder_layer2)\n",
        "\n",
        "  ### decoder ###\n",
        "  decoder_layer1 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(encoder_layer3)\n",
        "  decoder_layer2 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(decoder_layer1)\n",
        "  decoder_layer3 = layers.Conv2DTranspose(3, 4, strides = 2, activation=None, padding=\"same\")(decoder_layer2)\n",
        "\n",
        "  ### phys layer 1: apply periodic boundary conditions ###\n",
        "  apply_boundary_conditions = PeriodicBoundaryConditions()\n",
        "  periodic_boundary_conditions = apply_boundary_conditions(decoder_layer3)\n",
        "  boundary_conditions_layer = layers.Conv2D(3, 3, activation=None, padding=\"same\")(periodic_boundary_conditions)\n",
        "\n",
        "  ### phys layer 2: apply spatial derivatives as filters ###\n",
        "  spatial_derivatives_layer = layers.Conv2D(3,3, kernel_initializer=kernel_ijk, activation=None, padding = \"same\", strides = 1,trainable = False)(boundary_conditions_layer)\n",
        "\n",
        "  ### phys layer 3:\n",
        "  curl_layer = layers.Conv2D(3,3, activation=None, padding = \"same\", strides = 1,trainable = False)(spatial_derivatives_layer)\n",
        "\n",
        "  # model\n",
        "  model = keras.Model(input, curl_layer)\n",
        "  model.compile(optimizer=\"adam\", loss= \"mean_squared_error\")\n",
        "  model.summary()\n",
        "\n",
        "  # fit model and plot loss history\n",
        "  history = model.fit(train, train, epochs=epochs, batch_size=batch_size, validation_data=(validation, validation))\n",
        "  plot_history(history,True)\n",
        "\n",
        "  # model predictions\n",
        "  train_prediction = model.predict(train)\n",
        "  test_prediction = model.predict(test)\n",
        "\n",
        "  # RMSE training and test scores\n",
        "  RMSE(train_prediction, test_prediction, train, test)\n",
        "\n",
        "  # plot results\n",
        "  x, y = generate_scatterplot_indices(test.shape[1])\n",
        "  generate_plots(test_prediction, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename,epochs,dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample to 127 x 127, and add zero padding to make 128 x 128\n",
        "dimensions = 127\n",
        "\n",
        "# 70% training and validation dataset\n",
        "train_percent = 0.7\n",
        "output_filename = \"phys_cae_divergence.csv\"\n",
        "\n",
        "# load data\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# pad train and test sets\n",
        "train = np.pad(train, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)\n",
        "\n",
        "# 35% training and validation dataset\n",
        "train_percent = 0.35\n",
        "output_filename = \"phys_cae_divergence_5_train.csv\"\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# pad train and test sets\n",
        "train = np.pad(train, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)"
      ],
      "metadata": {
        "id": "ieqe9Lf25KiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxXTj7pzCwXs"
      },
      "source": [
        "**Phys CAE without the last layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OB4260KSC6Ey"
      },
      "outputs": [],
      "source": [
        "def phys_cae_no_layer_3(train, test, validation, epochs, batch_size, filenames_train, filenames_test, output_filename,dim):\n",
        "\n",
        "  # image contains zero padding in place of the ghost cells\n",
        "  input = layers.Input(shape=(train.shape[1],train.shape[2], train.shape[3]))\n",
        "\n",
        "  ### encoder ###\n",
        "  encoder_layer1 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(input)\n",
        "  encoder_layer2 = layers.Conv2D(6,  3, 2, activation='relu', padding='same')(encoder_layer1)\n",
        "  encoder_layer3 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(encoder_layer2)\n",
        "\n",
        "  ### decoder ###\n",
        "  decoder_layer1 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(encoder_layer3)\n",
        "  decoder_layer2 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(decoder_layer1)\n",
        "  decoder_layer3 = layers.Conv2DTranspose(3, 4, strides = 2, activation=None, padding=\"same\")(decoder_layer2)\n",
        "\n",
        "  ### phys layer 1: apply periodic boundary conditions ###\n",
        "  apply_boundary_conditions = PeriodicBoundaryConditions()\n",
        "  periodic_boundary_conditions = apply_boundary_conditions(decoder_layer3)\n",
        "  boundary_conditions_layer = layers.Conv2D(3, 3, activation=None, padding=\"same\")(periodic_boundary_conditions)\n",
        "\n",
        "  ### phys layer 2: apply spatial derivatives as filters ###\n",
        "  spatial_derivatives_layer = layers.Conv2D(3,3, kernel_initializer=kernel_ijk, activation=None, padding = \"same\", strides = 1,trainable = False)(boundary_conditions_layer)\n",
        "\n",
        "  # model\n",
        "  model = keras.Model(input, spatial_derivatives_layer) #curl_layer)\n",
        "  model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "  model.summary()\n",
        "\n",
        "  # fit model and plot loss history\n",
        "  history = model.fit(train, train, epochs=epochs, batch_size=batch_size, validation_data=(validation, validation))\n",
        "  plot_history(history,True)\n",
        "\n",
        "  # model predictions\n",
        "  train_prediction = model.predict(train)\n",
        "  test_prediction = model.predict(test)\n",
        "\n",
        "  # plot results\n",
        "  x, y = generate_scatterplot_indices(test.shape[1])\n",
        "  generate_plots(test_prediction, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename,epochs,dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample to 127 x 127, and add zero padding to make 128 x 128\n",
        "dimensions = 127\n",
        "\n",
        "# 70% training and test dataset\n",
        "train_percent = 0.7\n",
        "output_filename = \"phys_cae_divergence_no_layer3.csv\"\n",
        "\n",
        "# load data\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# pad train and test sets\n",
        "train = np.pad(train, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae_no_layer_3(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)\n",
        "\n",
        "# 35% training and validation dataset\n",
        "train_percent = 0.35\n",
        "output_filename = \"phys_cae_divergence_no_layer3_5_train.csv\"\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# pad train and test sets\n",
        "train = np.pad(train, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae_no_layer_3(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)"
      ],
      "metadata": {
        "id": "2CI1Mahh5pDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9RQpukC6u0"
      },
      "source": [
        "**Phys CAE without any padding (initial zero padding and BC layer)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv9ERI10DDse"
      },
      "outputs": [],
      "source": [
        "def phys_cae_no_bc(train, test, validation, epochs, batch_size, filenames_train, filenames_test, output_filename,dim):\n",
        "\n",
        "  # image contains zero padding in place of the ghost cells\n",
        "  input = layers.Input(shape=(train.shape[1],train.shape[2], train.shape[3]))\n",
        "\n",
        "  ### encoder ###\n",
        "  encoder_layer1 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(input)\n",
        "  encoder_layer2 = layers.Conv2D(6,  3, 2, activation='relu', padding='same')(encoder_layer1)\n",
        "  encoder_layer3 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(encoder_layer2)\n",
        "\n",
        "  ### decoder ###\n",
        "  decoder_layer1 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(encoder_layer3)\n",
        "  decoder_layer2 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(decoder_layer1)\n",
        "  decoder_layer3 = layers.Conv2DTranspose(3, 4, strides = 2, activation=None, padding=\"same\")(decoder_layer2)\n",
        "\n",
        "  ### phys layer 1: apply spatial derivatives as filters ###\n",
        "  spatial_derivatives_layer = layers.Conv2D(3,3, kernel_initializer=kernel_ijk, activation=None, padding = \"same\", strides = 1,trainable = False)(decoder_layer3) #(boundary_conditions_layer)\n",
        "\n",
        "  ### phys layer 2:\n",
        "  curl_layer = layers.Conv2D(3,3, activation=None, padding = \"same\", strides = 1,trainable = False)(spatial_derivatives_layer)\n",
        "\n",
        "  # model\n",
        "  model = keras.Model(input, curl_layer)\n",
        "  model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "  model.summary()\n",
        "\n",
        "  # fit model and plot loss history\n",
        "  history = model.fit(train, train, epochs=epochs, batch_size=batch_size, validation_data=(validation, validation))\n",
        "  plot_history(history,True)\n",
        "\n",
        "  # model predictions\n",
        "  train_prediction = model.predict(train)\n",
        "  test_prediction = model.predict(test)\n",
        "\n",
        "  # plot results\n",
        "  x, y = generate_scatterplot_indices(test.shape[1])\n",
        "  generate_plots(test_prediction, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename,epochs,dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# no zero padding\n",
        "dimensions = 128\n",
        "\n",
        "# 70% training and validation dataset\n",
        "train_percent = 0.7\n",
        "output_filename = \"phys_cae_divergence_no_padding.csv\"\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae_no_bc(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)\n",
        "\n",
        "# 35% training and validation dataset\n",
        "train_percent = 0.35\n",
        "output_filename = \"phys_cae_divergence_no_padding_5_train.csv\"\n",
        "\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae_no_bc(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)"
      ],
      "metadata": {
        "id": "TIWER8Ql6D0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or8GIaQ15oB3"
      },
      "source": [
        "**Plot Divergence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qhYFUGkjGtK"
      },
      "outputs": [],
      "source": [
        "input_df = pd.read_csv(filepath + \"input_divergence.csv\", header=None)\n",
        "input_divergence = input_df.loc[input_df[0] == 1100]\n",
        "\n",
        "labels = [\"PhysCAE, 70% training/validation\",\"PhysCAE, 35% training/validation\"]\n",
        "filenames = [\"phys_cae_divergence.csv\",\"phys_cae_divergence_5_train.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"CAE, padded fields\",\"PhysCAE\"]\n",
        "filenames = [\"cae_divergence_padded.csv\",\"phys_cae_divergence.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"CAE, padded fields, 35% training/validation\",\"PhysCAE, 35% training/validation\"]\n",
        "filenames = [\"cae_divergence_padded_5_train.csv\",\"phys_cae_divergence_5_train.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"CAE\",\"CAE, padded fields\"]\n",
        "filenames = [\"cae_divergence.csv\",\"cae_divergence_padded.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"PhysCAE\",\"PhysCAE, no curl layer\",\"PhysCAE, no bc layer\"]\n",
        "filenames = [\"phys_cae_divergence.csv\",\"phys_cae_divergence_no_layer3.csv\",\"phys_cae_divergence_no_padding.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"CAE\",\"CAE, padded fields\",\"PhysCAE\",\"PhysCAE, no curl layer\",\"PhysCAE, no bc layer\"]\n",
        "filenames = [\"cae_divergence.csv\",\"cae_divergence_padded.csv\",\"phys_cae_divergence_5_train.csv\",\"phys_cae_divergence_no_layer3.csv\",\"phys_cae_divergence_no_padding.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"PhysCAE, 35% training/validation\",\"PhysCAE, no curl layer, 35% training/validation\",\"PhysCAE, no bc layer, 35% training/validation\"]\n",
        "filenames = [\"phys_cae_divergence_5_train.csv\",\"phys_cae_divergence_no_layer3_5_train.csv\",\"phys_cae_divergence_no_padding_5_train.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)\n",
        "\n",
        "labels = [\"CAE, 35% training/validation\",\"CAE, padded fields, 35% training/validation\",\"PhysCAE, 35% training/validation\",\"PhysCAE, no curl layer, 35% training/validation\",\"PhysCAE, no bc layer, 35% training/validation\"]\n",
        "filenames = [\"cae_divergence_5_train.csv\",\"cae_divergence_padded_5_train.csv\",\"phys_cae_divergence_5_train.csv\",\"phys_cae_divergence_no_layer3_5_train.csv\",\"phys_cae_divergence_no_padding_5_train.csv\"]\n",
        "plot_mean_divergence(filenames,input_divergence,sorted(epochs), labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variation on Phys CAE using Three Layers for the Curl Operation**"
      ],
      "metadata": {
        "id": "4VHcAQg39HdW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGY1M7LhhA3S"
      },
      "outputs": [],
      "source": [
        "def kernel_i(shape, dtype=None):\n",
        "  delta = 1 # grid spacing\n",
        "  kernel = []\n",
        "\n",
        "  # (dA_z/dy - dA_y/dz)i\n",
        "  for i in range(0,shape[2]):\n",
        "    # dA_x/dx (set to zero because this is not used)\n",
        "    kernel.append([[0, 0, 0],\n",
        "                [0, 0, 0],\n",
        "                [0, 0, 0]])\n",
        "    #-dA_y/dz\n",
        "    kernel.append([[1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)],\n",
        "                [-1/(2*delta), 0, -1/(2*delta)]])\n",
        "    # dA_z/dy\n",
        "    kernel.append([[-1/(2*delta), -1/(2*delta), -1/(2*delta)],\n",
        "                [0, 0, 0],\n",
        "                [1/(2*delta), 1/(2*delta), 1/(2*delta)]])\n",
        "\n",
        "  kernel = np.asarray(kernel).reshape(shape[2],shape[3],shape[0],shape[1])\n",
        "  kernel = kernel.reshape(shape[0],shape[1],shape[2],shape[3])\n",
        "\n",
        "  return kernel\n",
        "\n",
        "def kernel_j(shape, dtype=None):\n",
        "  delta = 1 # grid spacing\n",
        "  kernel = []\n",
        "\n",
        "  #(dA_x/dz - dA_z/dx)j\n",
        "  for i in range(0,shape[2]):\n",
        "    # dA_x/dz\n",
        "    kernel.append([[-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)]])\n",
        "    # dA_y/dy (set to zero because this is not used)\n",
        "    kernel.append([[0, 0, 0],\n",
        "                [0, 0, 0],\n",
        "                [0, 0, 0]])\n",
        "    #-dA_z/dx\n",
        "    kernel.append([[1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)],\n",
        "                [1/(2*delta), 0, -1/(2*delta)]])\n",
        "\n",
        "  kernel = np.asarray(kernel).reshape(shape[2],shape[3],shape[0],shape[1])\n",
        "  kernel = kernel.reshape(shape[0],shape[1],shape[2],shape[3])\n",
        "\n",
        "  return kernel\n",
        "\n",
        "def kernel_k(shape, dtype=None):\n",
        "  delta = 1 # grid spacing\n",
        "  kernel = []\n",
        "\n",
        "  #(dA_y/dx - dA_x/dy)k\n",
        "  for i in range(0,shape[2]):\n",
        "    # -dA_x/dy\n",
        "    kernel.append([[1/(2*delta), 1/(2*delta), 1/(2*delta)],\n",
        "                [0, 0, 0],\n",
        "                [-1/(2*delta), -1/(2*delta), -1/(2*delta)]])\n",
        "    # dA_y/dx\n",
        "    kernel.append([[-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)],\n",
        "                [-1/(2*delta), 0, 1/(2*delta)]])\n",
        "    # dA_z/dz (set to zero because this is not used)\n",
        "    kernel.append([[0, 0, 0],\n",
        "                [0, 0, 0],\n",
        "                [0, 0, 0]])\n",
        "\n",
        "  kernel = np.asarray(kernel).reshape(shape[2],shape[3],shape[0],shape[1])\n",
        "  kernel = kernel.reshape(shape[0],shape[1],shape[2],shape[3])\n",
        "\n",
        "  return kernel\n",
        "\n",
        "def phys_cae_3_layer_curl(train, test, validation, epochs, batch_size, filenames_train, filenames_test, output_filename,dim):\n",
        "\n",
        "  # image contains zero padding in place of the ghost cells\n",
        "  input = layers.Input(shape=(train.shape[1],train.shape[2], train.shape[3]))\n",
        "\n",
        "  ### encoder ###\n",
        "  encoder_layer1 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(input)\n",
        "  encoder_layer2 = layers.Conv2D(6,  3, 2, activation='relu', padding='same')(encoder_layer1)\n",
        "  encoder_layer3 = layers.Conv2D(6, 3, 2, activation='relu', padding='same')(encoder_layer2)\n",
        "\n",
        "  ### decoder ###\n",
        "  decoder_layer1 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(encoder_layer3)\n",
        "  decoder_layer2 = layers.Conv2DTranspose(6, 3, strides=2, activation=\"relu\", padding=\"same\")(decoder_layer1)\n",
        "  decoder_layer3 = layers.Conv2DTranspose(3, 4, strides = 2, activation=None, padding=\"same\")(decoder_layer2)\n",
        "\n",
        "  ### phys layer 1: apply periodic boundary conditions ###\n",
        "  apply_boundary_conditions = PeriodicBoundaryConditions()\n",
        "  periodic_boundary_conditions = apply_boundary_conditions(decoder_layer3)\n",
        "  boundary_conditions_layer = layers.Conv2D(3, 3, activation=None, padding=\"same\")(periodic_boundary_conditions)\n",
        "\n",
        "  ### phys layer 2: apply spatial derivatives as filters ###\n",
        "  derivatives_i = layers.Conv2D(3,3, kernel_initializer=kernel_i, activation=None, padding = \"same\", strides = 1,trainable = False)(boundary_conditions_layer)\n",
        "  derivatives_j = layers.Conv2D(3,3, kernel_initializer=kernel_j, activation=None, padding = \"same\", strides = 1,trainable = False)(boundary_conditions_layer)\n",
        "  derivatives_k = layers.Conv2D(3,3, kernel_initializer=kernel_k, activation=None, padding = \"same\", strides = 1,trainable = False)(boundary_conditions_layer)\n",
        "\n",
        "  A_ijk = tf.convert_to_tensor([derivatives_i[:,:,:,0],derivatives_j[:,:,:,1],derivatives_k[:,:,:,2]])\n",
        "  A_ijk = tf.transpose(A_ijk, perm=[1, 2, 3, 0])\n",
        "\n",
        "  ### phys layer 3:\n",
        "  curl_layer = layers.Conv2D(3,3, activation=None, padding = \"same\", strides = 1,trainable = False)(A_ijk)\n",
        "\n",
        "  # model\n",
        "  model = keras.Model(input, curl_layer)\n",
        "  model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "  model.summary()\n",
        "\n",
        "  # fit model and plot loss history\n",
        "  history = model.fit(train, train, epochs=epochs, batch_size=batch_size, validation_data=(validation, validation))\n",
        "  plot_history(history,True)\n",
        "\n",
        "  # model predictions\n",
        "  train_prediction = model.predict(train)\n",
        "  test_prediction = model.predict(test)\n",
        "\n",
        "  # plot results\n",
        "  x, y = generate_scatterplot_indices(test.shape[1])\n",
        "  generate_plots(test_prediction, filenames_test, train.shape[0]+validation.shape[0], x,y,output_filename,epochs,dim)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downsample to 127 x 127, and add zero padding to make 128 x 128\n",
        "dimensions = 127\n",
        "train_percent = 0.7\n",
        "output_filename = \"phys_cae_3_layer_curl.csv\"\n",
        "\n",
        "# load data\n",
        "train, test, validation, filenames_train, filenames_test, filenames_validation = load_data_uv(filepath, filenames, train_percent, dimensions)\n",
        "\n",
        "# pad train and test sets\n",
        "train = np.pad(train, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "test = np.pad(test, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "validation = np.pad(validation, ((0,0),(1,0),(0,1),(0,0)), 'constant', constant_values=(0,0))\n",
        "\n",
        "for i in range(0,len(epochs)):\n",
        "  phys_cae_3_layer_curl(train, test, validation, epochs[i], batch_size, filenames_train, filenames_test,output_filename,dimensions)"
      ],
      "metadata": {
        "id": "Fg7QW_1K9Dzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jtGEZoUXAFq"
      },
      "source": [
        "**Test Case for Computing Divergence: compare the results from three variations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY8Pk3tbtLIj"
      },
      "outputs": [],
      "source": [
        "def compute_div(fields):\n",
        "  dF_dx, dF_dy, dF_dz = np.gradient(fields)\n",
        "\n",
        "  print(\"The sum of du_dx is \", np.sum(dF_dx))\n",
        "  print(\"The sum of dv_dy is \", np.sum(dF_dy))\n",
        "  print(\"The sum of dw_dz is \", np.sum(dF_dz))\n",
        "  print(\"The sum of du_dx + dv/dy + dw/dz is \", np.sum(dF_dx)+np.sum(dF_dy)+np.sum(dF_dz))\n",
        "\n",
        "  divergence = dF_dx + dF_dy + dF_dz\n",
        "\n",
        "  return divergence\n",
        "\n",
        "def compute_divergence_separate_components(fields):\n",
        "  dF_dx = np.gradient(fields[0])\n",
        "  dF_dy = np.gradient(fields[1])\n",
        "  dF_dz = np.gradient(fields[2])\n",
        "\n",
        "  print(\"The sum of du_dx is \", np.sum(dF_dx))\n",
        "  print(\"The sum of dv_dy is \", np.sum(dF_dy))\n",
        "  print(\"The sum of dw_dz is \", np.sum(dF_dz))\n",
        "  print(\"The sum of du_dx + dv/dy + dw/dz is \", np.sum(dF_dx)+np.sum(dF_dy)+np.sum(dF_dz))\n",
        "\n",
        "  divergence = dF_dx + dF_dy + dF_dz\n",
        "\n",
        "  return divergence\n",
        "\n",
        "def manually_compute_divergence(fields):\n",
        "  # based on: https://gist.github.com/GregTJ/24146e6731bae90a5a056c95cb94153d\n",
        "  shape = 512, 512\n",
        "  dims = 3\n",
        "\n",
        "  # compute the jacobian\n",
        "  partials = tuple(np.gradient(i) for i in fields)\n",
        "  jacobian = np.stack(partials).reshape(*(j := (dims,2)), *shape)\n",
        "\n",
        "  # extract divergence from the jacobian using trace\n",
        "  divergence = np.trace(jacobian)\n",
        "\n",
        "  return divergence\n",
        "\n",
        "filename = [\"fileu_100.dat\",\"filev_100.dat\"]\n",
        "\n",
        "dimensions = 512 # original dimensions of the field\n",
        "\n",
        "fieldu = pd.read_table(filepath + filename[0], delimiter=',',header=None).to_numpy()\n",
        "fieldv = pd.read_table(filepath + filename[1], delimiter=',',header=None).to_numpy()\n",
        "fieldw = np.zeros((dimensions,dimensions))\n",
        "\n",
        "fields = [fieldu,fieldv,fieldw]\n",
        "\n",
        "print(\"Method 1: Compute divergence as single gradient call\")\n",
        "divergence = compute_div(fields)\n",
        "sum_field1 = np.sum(divergence)/(512*512)\n",
        "\n",
        "filenames = [\"fileu_100.dat\",\"filev_100.dat\"]\n",
        "\n",
        "print(\"Method 2: Compute divergence as separate gradient calls\")\n",
        "divergence = compute_divergence_separate_components(fields)\n",
        "sum_field2 = np.sum(divergence)/(512*512)\n",
        "\n",
        "print(\"Method 3: Compute divergence manually\")\n",
        "divergence = manually_compute_divergence(fields)\n",
        "sum_field3 = np.sum(divergence)/(512*512)\n",
        "\n",
        "print(\"Method 1: the sum of the divergence of the vector field across all elements is: \", sum_field1)\n",
        "print(\"Method 2: the sum of the divergence of the vector field across all elements is: \", sum_field2)\n",
        "print(\"Method 3: the sum of the divergence of the vector field across all elements is: \", sum_field3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CrtyIIkdiPq"
      },
      "source": [
        "**Test Case for Periodic BCs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1QHYLPzHukd"
      },
      "outputs": [],
      "source": [
        "def bc_padding_test_case():\n",
        "  filenames = [\"fileu_100.dat\",\"filev_100.dat\",\n",
        "             \"fileu_200.dat\",\"filev_200.dat\",\n",
        "             \"fileu_300.dat\",\"filev_300.dat\",\n",
        "             \"fileu_400.dat\",\"filev_400.dat\",\n",
        "             \"fileu_500.dat\",\"filev_500.dat\"]\n",
        "\n",
        "  validation_percent = 0.4\n",
        "  dimensions = 4\n",
        "\n",
        "  inputs, test1,filenames_train, filenames_test = load_data_uv(filepath, filenames, validation_percent, dimensions)\n",
        "  ufield = inputs[0,:,:,0]\n",
        "\n",
        "  # this is the ufield\n",
        "  #[[ 0.00043109 -0.01509237 -0.03613735 -0.06296321]\n",
        "  # [ 0.002038   -0.00500754 -0.01705938 -0.03470955]\n",
        "  # [ 0.01829178  0.01953093  0.01641086  0.00802337]\n",
        "  # [ 0.0462379   0.05538551  0.06094921  0.06172691]]\n",
        "\n",
        "  # this is taken from the custom layer\n",
        "  padded = []\n",
        "  for i in range(0,inputs.shape[3]):\n",
        "    col = inputs[slice(None),:,:1,i] # the second index was changed to account for no zero padding\n",
        "    col = tf.transpose(col, perm=[1, 2, 0])\n",
        "\n",
        "    crop_input = inputs[slice(None),:,:,i] # the second and third indeces were changed to account for no zero padding\n",
        "    crop_input = tf.transpose(crop_input, perm=[1, 2, 0])\n",
        "\n",
        "    pad_col = tf.concat([crop_input,col],1)\n",
        "\n",
        "    row = pad_col[-1:,:,:]\n",
        "\n",
        "    pad_row = tf.concat([row, pad_col],0)\n",
        "\n",
        "    padded.append(pad_row)\n",
        "\n",
        "  padded = tf.transpose(tf.convert_to_tensor(padded), perm=[3, 1, 2, 0])\n",
        "\n",
        "  ufield_padded = padded[0,:,:,0]\n",
        "  print(ufield_padded)\n",
        "\n",
        "bc_padding_test_case()\n",
        "\n",
        "# this is the correct, expected padding for ufield\n",
        "#[[ 0.0462379   0.05538551  0.06094921  0.06172691  0.0462379]\n",
        "# [ 0.00043109 -0.01509237 -0.03613735 -0.06296321  0.00043109]\n",
        "# [ 0.002038   -0.00500754 -0.01705938 -0.03470955  0.002038]\n",
        "# [ 0.01829178  0.01953093  0.01641086  0.00802337  0.01829178]\n",
        "# [ 0.0462379   0.05538551  0.06094921  0.06172691  0.0462379]]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1C8nXhvXZWfMNPNg1xSCPFpfsHyovXzcV",
      "authorship_tag": "ABX9TyM+8/VeyOoSeDtKY/SO2v9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}